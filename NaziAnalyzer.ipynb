{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaziAnalyzer\n",
    "This notebook will analyze a dataset of Nazi profiles and tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(set(stopwords.words('spanish')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_stop_words = {'via', 'like', 'one', 'us', 'it\\'s', 'get', 'don\\'t', \n",
    "                   'would', 'want', '=', 'i\\'m', 'back', 'many', 'know', \n",
    "                  'new', 'time', 'see', 'go', 'think', 'today', 'say', 'day', \n",
    "                  'you\\'re', 'going', 'well', 'must', 'really', 'u', 'really', \n",
    "                  'lol', 'much', 'still', 'look', 'that\\'s', 'even', 'last', 'way'}\n",
    "stop_words.update(more_stop_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_ats(string):\n",
    "    # Remove any @s (e.g., replies) from a string.\n",
    "    return re.sub(r'(@\\S+\\s)', \"\", string)\n",
    "\n",
    "def remove_urls(string):\n",
    "    # Remove URLs from a string.\n",
    "    return re.sub(r'(https?://\\S*)', \"\", string)\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    # Remove punctuation from a string.\n",
    "    string = string.replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n",
    "    string = string.replace('(', ' ').replace(')', ' ').replace('\\[', ' ').replace('\\]', ' ')\n",
    "    string = string.replace('\\{', ' ').replace('\\}', ' ').replace('\\\\', ' ').replace('/', ' ')\n",
    "    string = string.replace(' -', ' ').replace('…', ' ').replace('\\\"', ' ').replace('&amp;', ' ')\n",
    "    string = string.replace('?', ' ').replace('!', ' ').replace('“', ' ').replace('”', ' ')\n",
    "    string = string.replace('   ', ' ').replace('  ', ' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    return string\n",
    "\n",
    "def test_1488(string):\n",
    "    # Detect 1488 and append the text \"FourteenEightyEight\"\n",
    "    #\n",
    "    # Match 14 and 88 only if no digit precedes nor follows.\n",
    "    # 14 and 88 may be separated by a single non-digit.\n",
    "    # e.g., matches: 1488, 14/88, 14.88, asdf1488jkl, 13-14-88a, a14 88b.\n",
    "    # e.g., non-matches: 14288, 14--88, 5551488555, 714/88.\n",
    "    if re.match(r\".*(?<!\\d)14[\\D]?88(?!\\d)\", string):\n",
    "        string += \" FourteenEightyEight\"\n",
    "        return string\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def remove_digits(string):\n",
    "    # Remove extraneous digits\n",
    "    string = re.sub(r'\\s(\\d+)\\W', ' ', string)\n",
    "    return string\n",
    "\n",
    "def clean_tweet(string):\n",
    "    # Detect 1488, remove digits, remove @s and URLs, and remove punctuation from a string.\n",
    "    string = test_1488(string)\n",
    "    string = remove_digits(string)\n",
    "    string = remove_ats(string)\n",
    "    string = remove_urls(string)\n",
    "    string = remove_punctuation(string)\n",
    "    return string\n",
    "\n",
    "def get_full_text(status):\n",
    "    # Get the full text from a status object.\n",
    "    try:\n",
    "        if 'retweeted_status' in status:\n",
    "            return status['retweeted_status']['full_text']\n",
    "        elif 'full_text' in status:\n",
    "            return status['full_text']\n",
    "        elif 'text' in status:\n",
    "            return status['text']\n",
    "        else:\n",
    "            print(\"Error in get_full_text(\" + str(status['id']) + \")\")\n",
    "            return \"\"\n",
    "    except BaseException as e:\n",
    "        print(\"Error in get_full_text(\" + str(status['id']) + \"): \", e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114240\n"
     ]
    }
   ],
   "source": [
    "json_data=open(\"data/nazi_tweets-2017-11-14-13_41_56.json\").read()\n",
    "#json_data=open(\"data/nazi_tweets-2017-11-14-12_27_33.json\").read()\n",
    "#json_data=open(\"data/nazi_tweets-2017-11-13-17_22_52.json\").read()\n",
    "#json_data=open(\"tweets-test-PaleoconWoman.json\").read()\n",
    "#json_data = open(\"tweet.json\").read()\n",
    "data = json.loads(json_data)\n",
    "\n",
    "tweet_data = [get_full_text(status) for status in data]\n",
    "clean_data = [clean_tweet(tweet) for tweet in tweet_data]\n",
    "print(len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(40,80):\n",
    "    print(clean_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white;7263\n",
      "people;5937\n",
      "trump;2393\n",
      "right;2188\n",
      "good;2117\n",
      "world;1820\n",
      "never;1753\n",
      "man;1706\n",
      "need;1693\n",
      "jews;1666\n",
      "black;1663\n",
      "race;1542\n",
      "country;1521\n",
      "great;1501\n",
      "make;1488\n",
      "whites;1464\n",
      "america;1376\n",
      "video;1361\n",
      "hate;1333\n",
      "take;1313\n",
      "#maga;1267\n",
      "stop;1246\n",
      "love;1230\n",
      "women;1224\n",
      "war;1224\n",
      "#whitegenocide;1179\n",
      "media;1174\n",
      "can't;1134\n",
      "left;1127\n",
      "please;1080\n",
      "president;1074\n",
      "real;1046\n",
      "years;1038\n",
      "every;1031\n",
      "nothing;1011\n",
      "racist;1002\n",
      "followers;1001\n",
      "men;988\n",
      "first;985\n",
      "fuck;982\n",
      "national;953\n",
      "always;931\n",
      "thing;928\n",
      "live;928\n",
      "jewish;922\n",
      "got;918\n",
      "shit;918\n",
      "also;911\n",
      "true;903\n",
      "children;899\n",
      "yes;898\n",
      "europe;886\n",
      "said;883\n",
      "keep;880\n",
      "ever;877\n",
      "fourteeneightyeight;876\n",
      "free;867\n",
      "news;867\n",
      "muslim;859\n",
      "could;850\n",
      "let;843\n",
      "another;839\n",
      "twitter;838\n",
      "truth;838\n",
      "hitler;830\n",
      "@youtube;827\n",
      "come;820\n",
      "antifa;812\n",
      "history;811\n",
      "year;809\n",
      "die;808\n",
      "support;807\n",
      "fight;807\n",
      "work;798\n",
      "thank;795\n",
      "american;784\n",
      "believe;784\n",
      "life;783\n",
      "god;773\n",
      "better;771\n",
      "made;767\n",
      "old;754\n",
      "jew;753\n",
      "oh;748\n",
      "muslims;748\n",
      "help;746\n",
      "call;745\n",
      "police;744\n",
      "says;730\n",
      "wrong;728\n",
      "give;716\n",
      "bad;715\n",
      "doesn't;713\n",
      "fucking;713\n",
      "hillary;710\n",
      "stats;701\n",
      "sure;698\n",
      "woman;691\n",
      "everyone;682\n",
      "south;681\n"
     ]
    }
   ],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for status in clean_data:\n",
    "    # Tokenize words, remove empty words, and lowercase all words.\n",
    "    new_tokens = [word.lower() for word in status.split(' ') if word != '' and word != ' ']\n",
    "    word_tokens.extend(new_tokens)\n",
    "\n",
    "filtered_words = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)    \n",
    "\n",
    "fdist = nltk.FreqDist(filtered_words)\n",
    "\n",
    "for word, frequency in fdist.most_common(100):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114240\n",
      "829145\n"
     ]
    }
   ],
   "source": [
    "print(str(len(clean_data)))\n",
    "print(str(len(filtered_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['words']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
